"""
–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –ø—Ä–æ—Å—Ç–æ–π LSTM-–º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–¥–∞—ë—Ç —Å–∏–≥–Ω–∞–ª—ã buy/sell/hold
–ø–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º OHLCV –¥–∞–Ω–Ω—ã–º –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º –∏–∑ data/tickers.
"""
import argparse
import json
import os
from pathlib import Path
import sys
from typing import Iterable, List, Sequence, Tuple

import joblib
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight

# –£–∫–∞–∑—ã–≤–∞–µ–º backend –¥–ª—è Keras (–æ–∂–∏–¥–∞–µ—Ç—Å—è tensorflow –∏–ª–∏ torch)
os.environ.setdefault("KERAS_BACKEND", "tensorflow")

try:
    from keras import callbacks, layers, models, optimizers
except ImportError as exc:  # pragma: no cover - —è–≤–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    raise SystemExit(
        "–ù–µ –Ω–∞–π–¥–µ–Ω backend –¥–ª—è Keras. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä: pip install 'tensorflow>=2.16'"
    ) from exc

PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))
    
import settings


PROJECT_ROOT = Path(__file__).resolve().parent.parent
DATA_DIR = PROJECT_ROOT / "data" / "tickers"
MODELS_DIR = PROJECT_ROOT / "models"
# –ë–∏–Ω–∞—Ä–Ω–∞—è —Å—Ö–µ–º–∞ –∫–ª–∞—Å—Å–æ–≤: 0 = sell, 1 = buy
CLASS_NAMES = {0: "sell", 1: "buy"}
DEFAULT_TICKERS = [item["name"] for item in settings.INSTRUMENTS]


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="–û–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–∏–≥–Ω–∞–ª–æ–≤ –ø–æ —Ü–µ–Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º"
    )
    parser.add_argument(
        "--timeframe",
        default="M30",
        help="–ò–º—è parquet-—Ñ–∞–π–ª–∞ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä H1, H4, D1)",
    )
    parser.add_argument(
        "--tickers",
        nargs="*",
        default=None,
        help="–°–ø–∏—Å–æ–∫ —Ç–∏–∫–µ—Ä–æ–≤. –ï—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω, –±–µ—Ä—ë–º –≤—Å–µ –∏–∑ settings.INSTRUMENTS",
    )
    parser.add_argument(
        "--lookback",
        type=int,
        default=64,
        help="–î–ª–∏–Ω–∞ –æ–∫–Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏ (–∫–æ–ª-–≤–æ –±–∞—Ä–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)",
    )
    parser.add_argument(
        "--horizon",
        type=int,
        default=4,
        help="–ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ –≤ –±–∞—Ä–∞—Ö (—á–µ—Ä–µ–∑ —Å–∫–æ–ª—å–∫–æ –±–∞—Ä–æ–≤ –æ—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç)",
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.003,
        help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –¥–ª—è —Å–∏–≥–Ω–∞–ª–∞ buy/sell (–Ω–∞–ø—Ä–∏–º–µ—Ä 0.003 = 0.3%)",
    )
    parser.add_argument(
        "--val-size",
        type=float,
        default=0.2,
        help="–î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–±–µ—Ä—ë—Ç—Å—è —Å —Ö–≤–æ—Å—Ç–∞ –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —á–µ—Å—Ç–Ω–æ—Å—Ç–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏)",
    )
    parser.add_argument(
        "--test-size",
        type=float,
        default=0.1,
        help="–î–æ–ª—è —Ç–µ—Å—Ç–∞ (—Å–∞–º—ã–π —Å–≤–µ–∂–∏–π —Ö–≤–æ—Å—Ç –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏)",
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=25,
        help="–ö–æ–ª-–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=256,
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞",
    )
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=1e-3,
        help="–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è Adam",
    )
    parser.add_argument(
        "--max-rows",
        type=int,
        default=None,
        help="–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å —á–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ (–¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö —Ç–µ—Å—Ç–æ–≤)",
    )
    parser.add_argument(
        "--model-path",
        type=Path,
        default=MODELS_DIR / "signal_model.keras",
        help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å",
    )
    parser.add_argument(
        "--scaler-path",
        type=Path,
        default=MODELS_DIR / "signal_model_scaler.pkl",
        help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å StandardScaler",
    )
    parser.add_argument(
        "--meta-path",
        type=Path,
        default=MODELS_DIR / "signal_model_meta.json",
        help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (—Ñ–∏—á–∏, –º—ç–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤)",
    )
    parser.add_argument(
        "--plot-path",
        type=Path,
        default=MODELS_DIR / "signal_model_learning_curve.png",
        help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è (loss/accuracy)",
    )
    return parser.parse_args()


def load_ticker_df(ticker: str, timeframe: str, max_rows: int | None) -> pd.DataFrame:
    """–ß–∏—Ç–∞–µ–º parquet –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ –¥–ª—è —Ç–∏–∫–µ—Ä–∞ (—Å–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –æ—Ç–º–µ—á–∞–µ–º —Ç–∏–∫–µ—Ä)."""
    file_path = DATA_DIR / ticker / f"{timeframe}.parquet"
    if not file_path.exists():
        raise FileNotFoundError(f"–ù–µ—Ç —Ñ–∞–π–ª–∞ {file_path}")

    df = pd.read_parquet(file_path)
    df = df.sort_index()
    if max_rows:
        df = df.tail(max_rows)

    df["ticker"] = ticker
    df.index = pd.to_datetime(df.index)
    return df


def concat_panel(tickers: Sequence[str], timeframe: str, max_rows: int | None) -> pd.DataFrame:
    """–û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤ –≤ –æ–¥–∏–Ω –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –∏ —á–∏—Å—Ç–∏–º –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏/NaN."""
    frames = []
    for ticker in tickers:
        try:
            frames.append(load_ticker_df(ticker, timeframe, max_rows))
        except FileNotFoundError as exc:
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {ticker}: {exc}")

    if not frames:
        raise SystemExit("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–∏–Ω —Ç–∏–∫–µ—Ä.")

    df = pd.concat(frames).sort_index()
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.dropna()

    ticker_to_id = {name: idx for idx, name in enumerate(sorted(tickers))}
    df["ticker_id"] = df["ticker"].map(ticker_to_id)
    return df


def add_labels(df: pd.DataFrame, horizon: int, threshold: float) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–æ–ª–±–µ—Ü signal (–±–∏–Ω–∞—Ä–Ω–æ):
    0 = sell, 1 = buy.
    –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (|ret| <= threshold) —É–¥–∞–ª—è—é—Ç—Å—è, —á—Ç–æ–±—ã –Ω–µ –ø–ª–æ–¥–∏—Ç—å hold.
    """
    df = df.copy()
    df["future_close"] = df["close"].shift(-horizon)
    df["future_ret"] = (df["future_close"] - df["close"]) / df["close"]
    # –ú–∞—Å–∫–∏ –¥–ª—è –±–∞–π/—Å–µ–ª–ª; –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º
    mask_buy = df["future_ret"] > threshold
    mask_sell = df["future_ret"] < -threshold
    df = df[mask_buy | mask_sell]
    df["signal"] = np.where(df["future_ret"] > threshold, 1, 0)
    df = df.dropna(subset=["future_ret", "signal"])
    if df.empty:
        raise SystemExit("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ threshold –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã. –£–º–µ–Ω—å—à–∏—Ç–µ threshold –∏–ª–∏ –≤–æ–∑—å–º–∏—Ç–µ –±–æ–ª—å—à–µ —Å—Ç—Ä–æ–∫.")
    return df


def build_sequences(
    feature_values: np.ndarray, labels: np.ndarray, lookback: int
) -> Tuple[np.ndarray, np.ndarray]:
    """–°—Ç—Ä–æ–∏–º –æ–∫–Ω–∞ –¥–ª–∏–Ω—ã lookback, –º–µ—Ç–∫–∞ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–≤–µ—á–µ –æ–∫–Ω–∞."""
    sequences: List[np.ndarray] = []
    seq_labels: List[int] = []

    for end_idx in range(lookback - 1, len(feature_values)):
        label = labels[end_idx]
        if np.isnan(label):
            continue
        window = feature_values[end_idx - lookback + 1 : end_idx + 1]
        if np.any(np.isnan(window)):
            continue
        sequences.append(window)
        seq_labels.append(int(label))

    if not sequences:
        raise SystemExit("–ü–æ—Å–ª–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ–∫–æ–Ω –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")

    X = np.asarray(sequences, dtype=np.float32)
    y = np.asarray(seq_labels, dtype=np.int64)
    return X, y


def chronological_split(
    X: np.ndarray, y: np.ndarray, val_size: float, test_size: float
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    –ß–µ—Å—Ç–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏: train -> val -> test (—Å–∞–º—ã–π —Å–≤–µ–∂–∏–π).
    """
    if val_size + test_size >= 1:
        raise SystemExit("–°—É–º–º–∞ val_size –∏ test_size –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å < 1.")

    n = len(X)
    train_end = int(n * (1 - val_size - test_size))
    val_end = int(n * (1 - test_size))
    train_end = max(train_end, 1)
    val_end = max(val_end, train_end + 1)

    X_train, y_train = X[:train_end], y[:train_end]
    X_val, y_val = X[train_end:val_end], y[train_end:val_end]
    X_test, y_test = X[val_end:], y[val_end:]

    if len(X_val) == 0 or len(X_test) == 0:
        raise SystemExit("–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è val/test –ø–æ—Å–ª–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è.")

    return X_train, X_val, X_test, y_train, y_val, y_test


def scale_sequences(
    X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray | None = None
) -> Tuple[np.ndarray, np.ndarray, np.ndarray | None, StandardScaler]:
    """–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ train –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º scaler (val/test —á–µ—Ä–µ–∑ —Ç–æ—Ç –∂–µ –º–∞—Å—à—Ç–∞–±)."""
    scaler = StandardScaler()
    flat_train = X_train.reshape(len(X_train), -1)
    scaler.fit(flat_train)

    def _transform(arr: np.ndarray) -> np.ndarray:
        flat = arr.reshape(len(arr), -1)
        return scaler.transform(flat).reshape(arr.shape)

    X_train_scaled = _transform(X_train)
    X_val_scaled = _transform(X_val)
    X_test_scaled = _transform(X_test) if X_test is not None else None
    return X_train_scaled, X_val_scaled, X_test_scaled, scaler


def build_model(lookback: int, n_features: int, learning_rate: float) -> models.Model:
    """–ü—Ä–æ—Å—Ç–∞—è –¥–≤—É—Ö—Å–ª–æ–π–Ω–∞—è LSTM —Å dropout –ø–æ–¥ –±–∏–Ω–∞—Ä–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é."""
    model = models.Sequential(
        [
            layers.Input(shape=(lookback, n_features)),
            layers.Masking(mask_value=0.0),
            layers.LSTM(64, return_sequences=True),
            layers.Dropout(0.2),
            layers.LSTM(32),
            layers.Dense(32, activation="relu"),
            layers.Dropout(0.2),
            layers.Dense(len(CLASS_NAMES), activation="softmax"),
        ]
    )
    model.compile(
        optimizer=optimizers.Adam(learning_rate=learning_rate),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model


def fit_model(
    model: models.Model,
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_val: np.ndarray,
    y_val: np.ndarray,
    epochs: int,
    batch_size: int,
) -> callbacks.History:
    """
    –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å early stopping –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ–º lr, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º history.
    –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ —Å—á–∏—Ç–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, —á—Ç–æ–±—ã –∫–æ–º–ø–µ–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å.
    """
    class_weights = compute_class_weight(
        class_weight="balanced", classes=np.unique(y_train), y=y_train
    )
    class_weights_dict = {cls: float(w) for cls, w in zip(np.unique(y_train), class_weights)}

    early_stop = callbacks.EarlyStopping(
        monitor="val_loss", patience=5, restore_best_weights=True, verbose=1
    )
    reduce_lr = callbacks.ReduceLROnPlateau(
        monitor="val_loss", factor=0.5, patience=2, min_lr=1e-5, verbose=1
    )

    history = model.fit(
        X_train,
        y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        class_weight=class_weights_dict,
        callbacks=[early_stop, reduce_lr],
        verbose=2,
    )
    return history


def plot_history(history: callbacks.History, out_path: Path) -> None:
    """–°—Ç—Ä–æ–∏–º –∫—Ä–∏–≤—ã–µ loss/accuracy –¥–ª—è train/val –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º PNG."""
    hist = history.history
    plt.figure(figsize=(10, 4))

    plt.subplot(1, 2, 1)
    plt.plot(hist.get("loss", []), label="train_loss")
    plt.plot(hist.get("val_loss", []), label="val_loss")
    plt.title("Loss")
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(hist.get("accuracy", []), label="train_acc")
    plt.plot(hist.get("val_accuracy", []), label="val_acc")
    plt.title("Accuracy")
    plt.legend()

    plt.tight_layout()
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=150)
    plt.close()


def collect_feature_columns(df: pd.DataFrame) -> List[str]:
    """–°–ø–∏—Å–æ–∫ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏—Å–∫–ª—é—á–∞—è —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è."""
    exclude = {"signal", "future_close", "future_ret", "ticker"}
    numeric_cols = []
    for col in df.columns:
        if col in exclude:
            continue
        if pd.api.types.is_numeric_dtype(df[col]):
            numeric_cols.append(col)
    return sorted(numeric_cols)


def main() -> None:
    args = parse_args()
    tickers = args.tickers or DEFAULT_TICKERS

    MODELS_DIR.mkdir(parents=True, exist_ok=True)

    # 1) –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
    print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ: {len(tickers)} —Ç–∏–∫–µ—Ä–æ–≤, tf={args.timeframe}")
    df = concat_panel(tickers, args.timeframe, args.max_rows)
    # 2) –†–∞–∑–º–µ—Ç–∫–∞: –±–∏–Ω–∞—Ä–Ω–æ buy/sell, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º
    df = add_labels(df, horizon=args.horizon, threshold=args.threshold)

    feature_cols = collect_feature_columns(df)
    if not feature_cols:
        raise SystemExit("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")

    features = df[feature_cols].to_numpy(dtype=np.float32)
    labels = df["signal"].to_numpy(dtype=np.float32)

    # 3) –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–ø–ª–∏—Ç train/val/test
    X, y = build_sequences(features, labels, lookback=args.lookback)
    X_train, X_val, X_test, y_train, y_val, y_test = chronological_split(
        X, y, val_size=args.val_size, test_size=args.test_size
    )
    # 4) –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ train
    X_train, X_val, X_test, scaler = scale_sequences(X_train, X_val, X_test)

    print(
        f"üßæ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(X_train)} —Å—ç–º–ø–ª–∞—Ö, –≤–∞–ª–∏–¥–∞—Ü–∏—è {len(X_val)}, "
        f"—Ñ–∏—á: {len(feature_cols)}"
    )
    # 5) –°–±–æ—Ä–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = build_model(args.lookback, n_features=len(feature_cols), learning_rate=args.learning_rate)
    history = fit_model(
        model,
        X_train,
        y_train,
        X_val,
        y_val,
        epochs=args.epochs,
        batch_size=args.batch_size,
    )

    # 6) –û—Ü–µ–Ω–∫–∞ –Ω–∞ val/test
    val_pred = model.predict(X_val, verbose=0).argmax(axis=1)
    test_pred = model.predict(X_test, verbose=0).argmax(axis=1)
    report = classification_report(
        y_val, val_pred, target_names=[CLASS_NAMES[i] for i in sorted(CLASS_NAMES)], output_dict=True
    )
    test_report = classification_report(
        y_test, test_pred, target_names=[CLASS_NAMES[i] for i in sorted(CLASS_NAMES)], output_dict=True
    )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
    model.save(args.model_path)
    joblib.dump(scaler, args.scaler_path)
    with args.meta_path.open("w", encoding="utf-8") as f:
        json.dump(
            {
                "feature_columns": feature_cols,
                "class_mapping": CLASS_NAMES,
                "timeframe": args.timeframe,
                "lookback": args.lookback,
                "horizon": args.horizon,
                "threshold": args.threshold,
                "val_size": args.val_size,
                "test_size": args.test_size,
                "tickers": tickers,
                "val_report": report,
                "test_report": test_report,
            },
            f,
            ensure_ascii=False,
            indent=2,
        )

    # 8) –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç—á—ë—Ç
    plot_history(history, args.plot_path)

    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {args.model_path}")
    print(f"‚úÖ Scaler —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {args.scaler_path}")
    print(f"üñºÔ∏è  –ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {args.plot_path}")
    print(
        f"üìä Val accuracy: {report['accuracy']:.3f} | "
        f"buy F1: {report['buy']['f1-score']:.3f}, "
        f"sell F1: {report['sell']['f1-score']:.3f}"
    )
    print(
        f"üß™ Test accuracy: {test_report['accuracy']:.3f} | "
        f"buy F1: {test_report['buy']['f1-score']:.3f}, "
        f"sell F1: {test_report['sell']['f1-score']:.3f}"
    )


if __name__ == "__main__":
    main()

